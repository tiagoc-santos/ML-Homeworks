{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, matplotlib.pyplot as plt, numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('heart-disease.csv')\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Compare the performance of a kNN with k = 5 and a Naïve Bayes with Gaussian assumption (consider all remaining parameters as default):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> a. Plot two boxplots with the fold accuracies for each classifier. Is there one more stable than the other regarding performance? Why do you think that is the case? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classify the data set using kNN and Naive Bayes\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "nb = GaussianNB()\n",
    "\n",
    "## Evaluate by cross-validation\n",
    "knn_scores = cross_val_score(knn, X, y, cv=folds)\n",
    "nb_scores = cross_val_score(nb, X, y, cv=folds)\n",
    "\n",
    "print(f\"Scaled kNN Accuracy: {knn_scores.mean():.3f}\")\n",
    "print(f\"Scaled Naive Bayes Accuracy: {nb_scores.mean():.3f}\")\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([knn_scores, nb_scores], tick_labels=['kNN (k = 5)', 'Naive Bayes (Gaussian)'] )\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Accuracy: kNN vs Naive Bayes')\n",
    "plt.grid(axis = 'y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naïve Bayes Gaussian model appears to be more stable when it comes to accuracy compared to the kNN model. This might be because of the high-dimensionality of the data set (13 features). \n",
    "\n",
    "The kNN model is distance-based, which means it relies on distance between data points to identify the relationship between them and patterns in the data. For high-dimensional spaces, the data becomes too sparse and the distance between instances more insignificant, as they all become very far from one another. Therefore, the kNN model struggles to find “closeness” between data instances and suffers in accuracy scores.\n",
    "\n",
    "On the other hand, by assuming independence between features, the Naïve Bayes model becomes simpler and may generalize better, leading to similar accuracies across all folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b. Report the accuracy of both models, this time scaling the data with a Min-Max scaler before training the models. Explain the impact that this preprocessing step has on the performance of each model, providing an explanation for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "nb = GaussianNB()\n",
    "\n",
    "X_scaled = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "knn_scaled_scores = cross_val_score(knn, X_scaled, y, cv=folds)\n",
    "nb_scaled_scores = cross_val_score(nb, X_scaled, y, cv=folds)\n",
    "\n",
    "print(f\"Scaled kNN Accuracy: {knn_scaled_scores.mean():.3f}\")\n",
    "print(f\"Scaled Naive Bayes Accuracy: {nb_scaled_scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features in the data set have different scales, they will end up impacting the results in the kNN model with different significance, due to it being a distance-based model. This introduces a bias in the data set. However,  the Naïve Bayes model is not affected by this problem because it assumes independence between the features. \n",
    "\n",
    "By applying Min-Max Scaling during the preprocessing stage, we aim to reduce the bias in the kNN algorithm by normalizing all features under the same scale (usually [0, 1]). Now, they should all have the same impact in the distance calculation.\n",
    "\n",
    "The impact in the accuracy after applying Min-Max Scaling proves the theory, as we can see that the accuracy for the kNN model increased significantly and the accuracy for the Naïve Bayes model stayed pretty much the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c. Using scipy, test the hypothesis “the kNN model is statistically superior to Naïve Bayes regarding accuracy”, asserting whether it is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a paired t-test\n",
    "t_stat, p_val = stats.ttest_rel(knn_scores, nb_scores, alternative = 'greater');\n",
    "alpha = 0.05\n",
    "\n",
    "# Print the results\n",
    "if p_val < alpha:\n",
    "    print(p_val, \"<\", alpha)\n",
    "    print(\"Reject the null hypothesis\")\n",
    "    print(\"kNN is statistcally superior to Naive Bayes regarding accuracy\")\n",
    "else:\n",
    "    print(p_val, \">=\", alpha)\n",
    "    print(\"Fail to reject the null hypothesis\")\n",
    "    print(\"kNN is not statistcally superior to Naive Bayes regarding accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using a 80-20 train-test split, vary the number of neighbors of a kNN classifier using k = {1, 5, 10, 20, 30}. Additionally, for each, train one classifier using uniform weights and distance weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> a.  Plot the train and test accuracy for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "k_values = [1, 5, 10, 20, 30]\n",
    "\n",
    "train_accuracies_uniform = []\n",
    "test_accuracies_uniform = []\n",
    "train_accuracies_distance = []\n",
    "test_accuracies_distance = []\n",
    "\n",
    "# Train and test the model for different k values\n",
    "for k in k_values:\n",
    "    # Train and test the model for uniform weights\n",
    "    knn_uniform = KNeighborsClassifier(n_neighbors=k, weights='uniform')\n",
    "    knn_uniform.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc_uniform = accuracy_score(y_train, knn_uniform.predict(X_train))\n",
    "    test_acc_uniform = accuracy_score(y_test, knn_uniform.predict(X_test))\n",
    "    \n",
    "    train_accuracies_uniform.append(train_acc_uniform)\n",
    "    test_accuracies_uniform.append(test_acc_uniform)\n",
    "    \n",
    "    # Train and test the model for distance weights\n",
    "    knn_distance = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "    knn_distance.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc_distance = accuracy_score(y_train, knn_distance.predict(X_train))\n",
    "    test_acc_distance = accuracy_score(y_test, knn_distance.predict(X_test))\n",
    "    \n",
    "    train_accuracies_distance.append(train_acc_distance)\n",
    "    test_accuracies_distance.append(test_acc_distance)\n",
    "\n",
    "\n",
    "#Plot the figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(k_values, train_accuracies_uniform, label='Train Accuracy (Uniform)')\n",
    "plt.plot(k_values, test_accuracies_uniform, label='Test Accuracy (Uniform)')\n",
    "plt.plot(k_values, train_accuracies_distance, label='Train Accuracy (Distance)')\n",
    "plt.plot(k_values, test_accuracies_distance, label='Test Accuracy (Distance)')\n",
    "\n",
    "plt.title('Train and Test Accuracy for varying k: Uniform and Distance weights')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b. Explain the impact of increasing the neighbors on the generalization ability of the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By increasing the number of neighbors in aa kNN model, it becomes a better at generalization. This is true because we look at a bigger portion of the population when deciding the output value of a point, instead of just the few closest points. Therefore, the model gets a better view of the data and patterns instead of focusing just on smaller areas and most probably catching noise during training.\n",
    "\n",
    "The results confirm this hypothesis. We can see that for smaller values of k in the graph, the accuracy of the uniform weights training is very high, opposed to the small scores in the corresponding uniform weights test. The model is suffering from overfitting caused by the noise caught associated with only looking at the few closest neighbors. As we increase the k value, the test accuracy increases \n",
    "proving the also increasing generalization capability of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Considering the unique properties of the heart-disease.csv dataset, identify two possible difficulties of the Naïve Bayes model used in the previous exercises when learning from the given dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, when we use Naïve Bayes we are assuming that all variables are independent from each other , which may not be true for our specific datatset. For example, features such as \"cholesterol in mg/d\", \"resting blood pressure\" and \"age\" are often not independent, as age is usually correlated with both these features. Ignoring this correlation may lead to misjudges by our model. For instance, high cholesterol and high blood pressure together could be stronger indicators of heart disease than when analyzed independently, something that Naïve Bayes wouldn't be able to catch.\n",
    "\n",
    "Secondly, with Naïve Bayes we assume that all features are equally important, which may not reflect reality. Certain features, such as \"chest pain\" or \"maximum heart rate achieved\" may hold more predictive weight than \"resting electrocardiographic results\", when it comes to diagnose a heart disease."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
